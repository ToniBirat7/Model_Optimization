{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7375ec",
   "metadata": {},
   "source": [
    "## **Model optimization has 4 big layers**\n",
    "\n",
    "### **Layer 1 — Mathematical Foundations (The Theory Level)**\n",
    "\n",
    "### **Layer 2 — Architectural Optimization (Designing Efficient Models)**\n",
    "\n",
    "### **Layer 3 — Systems-Level Optimization (Runtime, GPU, compilation)**\n",
    "\n",
    "### **Layer 4 — Deployment Optimization (Quantization, pruning, distillation)**\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254db52f",
   "metadata": {},
   "source": [
    "# **FULL ROADMAP: Become a Model Optimization Expert**\n",
    "\n",
    "## 1. PREREQUISITES (You MUST know these first)\n",
    "\n",
    "### A. Deep Learning Foundations\n",
    "\n",
    "- Backpropagation deeply (Jacobian, Hessian intuition)\n",
    "- Initialization strategies (Xavier, Kaiming, LSUV)\n",
    "- Activation functions & saturation\n",
    "- Loss surfaces, local minima, flat vs sharp minima\n",
    "\n",
    "### B. GPU Basics\n",
    "\n",
    "You MUST understand:\n",
    "\n",
    "- How CUDA cores work\n",
    "- What makes GPU faster than CPU\n",
    "- Memory hierarchy:\n",
    "\n",
    "  - global memory\n",
    "  - shared memory\n",
    "  - L2 cache\n",
    "  - registers\n",
    "\n",
    "### C. PyTorch Internals\n",
    "\n",
    "- Autograd graph\n",
    "- Hook functions\n",
    "- Custom backward\n",
    "- TorchScript basics\n",
    "\n",
    "If you are weak in any of these, I can give you a crash course.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bed601",
   "metadata": {},
   "source": [
    "# **2. MODEL OPTIMIZATION ROADMAP (HIERARCHY)**\n",
    "\n",
    "## LEVEL 1 — Foundations of Model Efficiency (Math + Theory)\n",
    "\n",
    "### 1. Parameter efficiency\n",
    "\n",
    "- Low-rank approximations\n",
    "\n",
    "- SVD compression\n",
    "\n",
    "- Weight tying\n",
    "\n",
    "- Bottleneck layers\n",
    "\n",
    "- Toeplitz/Circulant matrices\n",
    "\n",
    "- Grouped convolutions\n",
    "\n",
    "- Depthwise separable convolutions\n",
    "\n",
    "### 2. Why big models are slow\n",
    "\n",
    "- FLOPs\n",
    "\n",
    "- Memory bandwidth bottlenecks\n",
    "\n",
    "- Compute vs memory-bound models\n",
    "\n",
    "- Kernel fusion\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0facc7ee",
   "metadata": {},
   "source": [
    "## LEVEL 2 — Architecture Design Optimization\n",
    "\n",
    "Here you study how efficient architectures are designed.\n",
    "\n",
    "### Learn these model families deeply:\n",
    "\n",
    "| Model Type             | What to study                              |\n",
    "| ---------------------- | ------------------------------------------ |\n",
    "| **MobileNetV1/V2/V3**  | Depthwise convolutions, inverted residuals |\n",
    "| **EfficientNet**       | Compound scaling                           |\n",
    "| **ShuffleNet**         | Channel shuffle                            |\n",
    "| **SqueezeNet**         | “Fire” modules                             |\n",
    "| **DistilBERT**         | Layer dropping, token pruning              |\n",
    "| **LLaMA architecture** | Rotary embeddings, RMSNorm, SwiGLU         |\n",
    "\n",
    "**Skills you gain**\n",
    "\n",
    "- How to design an efficient block\n",
    "\n",
    "- How to reduce FLOPs without reducing accuracy\n",
    "\n",
    "- How to design models for mobile vs servers\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430358d",
   "metadata": {},
   "source": [
    "## LEVEL 3 — Training-Time Optimization\n",
    "\n",
    "Here you optimize the training itself, not only the final model.\n",
    "\n",
    "### **1. Techniques**\n",
    "\n",
    "- Mixed precision training (FP16, BF16)\n",
    "\n",
    "- Gradient checkpointing\n",
    "\n",
    "- Gradient accumulation\n",
    "\n",
    "- ZeRO optimizers (DeepSpeed)\n",
    "\n",
    "- Activation recomputation\n",
    "\n",
    "- FlashAttention\n",
    "\n",
    "- Low-memory optimizers (Adafactor)\n",
    "\n",
    "### **2. Distributed training**\n",
    "\n",
    "- Data parallelism vs model parallelism vs pipeline parallelism\n",
    "\n",
    "- Fully Sharded Data Parallel (FSDP)\n",
    "\n",
    "- Tensor parallelism (Megatron-LM)\n",
    "\n",
    "This directly improves training cost, memory, and training time.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2273e",
   "metadata": {},
   "source": [
    "## LEVEL 4 — Model Compression Techniques (Deployment Level)\n",
    "\n",
    "This is what most people call “model optimization”, but it's only one part.\n",
    "\n",
    "### **A. Quantization**\n",
    "\n",
    "You need to understand:\n",
    "\n",
    "- Post-training quantization (PTQ)\n",
    "\n",
    "- Quantization-aware training (QAT)\n",
    "\n",
    "- Weight-only quantization\n",
    "\n",
    "- 8-bit, 4-bit, 2-bit, binary networks\n",
    "\n",
    "- GPTQ, AWQ, SmoothQuant\n",
    "\n",
    "- KV-cache quantization for transformers\n",
    "\n",
    "### **B. Pruning**\n",
    "\n",
    "Types:\n",
    "\n",
    "- Magnitude pruning\n",
    "\n",
    "- Structured pruning (channels / attention heads)\n",
    "\n",
    "- Movement pruning\n",
    "\n",
    "- Lottery ticket hypothesis\n",
    "\n",
    "- N:M sparsity (Ampere GPUs support 2:4 sparsity)\n",
    "\n",
    "### **C. Knowledge Distillation**\n",
    "\n",
    "- Soft targets\n",
    "\n",
    "- Matching hidden states\n",
    "\n",
    "- Patient Knowledge Distillation\n",
    "\n",
    "- Layer dropping\n",
    "\n",
    "- Distillation for LLMs\n",
    "\n",
    "- Distillation for vision models\n",
    "\n",
    "### **D. Low-Rank Factorization**\n",
    "\n",
    "- LoRA\n",
    "\n",
    "- QLoRA\n",
    "\n",
    "- LLaMA adapters\n",
    "\n",
    "- Linear weight decomposition\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12afef",
   "metadata": {},
   "source": [
    "## LEVEL 5 — Runtime & Systems-Level Optimization\n",
    "\n",
    "Here you become a **systems + ML expert**.\n",
    "\n",
    "You need to learn:\n",
    "\n",
    "### **A. Compilers & Tensor Runtimes**\n",
    "\n",
    "- ONNX Runtime\n",
    "\n",
    "- Torch-TensorRT\n",
    "\n",
    "- TensorRT (for Nvidia)\n",
    "\n",
    "- Apache TVM\n",
    "\n",
    "- XLA (Google)\n",
    "\n",
    "- AITemplate (Meta)\n",
    "\n",
    "- OpenVINO (Intel)\n",
    "\n",
    "Learn how ops get fused, how kernels are optimized.\n",
    "\n",
    "### **B. Memory optimization**\n",
    "\n",
    "- KV-Cache optimization for transformers\n",
    "\n",
    "- FlashAttention & fused kernels\n",
    "\n",
    "- CUDA kernel fusion\n",
    "\n",
    "- Operator fusion (bias + activation + matmul fusions)\n",
    "\n",
    "### **C. Profiling on GPU**\n",
    "\n",
    "You MUST master:\n",
    "\n",
    "- Nsight Systems\n",
    "\n",
    "- Nsight Compute\n",
    "\n",
    "- PyTorch Profiler\n",
    "\n",
    "- TensorBoard Profiling\n",
    "\n",
    "You will learn to inspect:\n",
    "\n",
    "- kernel launches\n",
    "\n",
    "- flop utilization\n",
    "\n",
    "- memory stalls\n",
    "\n",
    "- warp divergence\n",
    "\n",
    "- shared memory bank conflicts\n",
    "\n",
    "This is where **real optimization** happens.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0500e93",
   "metadata": {},
   "source": [
    "## LEVEL 6 — Advanced Topics (Mastery)\n",
    "\n",
    "If you want to become a top expert, learn:\n",
    "\n",
    "### **1. Compiler IRs**\n",
    "\n",
    "- MLIR\n",
    "\n",
    "- Triton language (write custom GPU kernels)\n",
    "\n",
    "### **2. Custom CUDA kernels**\n",
    "\n",
    "- Matrix multiplication kernel\n",
    "\n",
    "- Attention kernel\n",
    "\n",
    "- LayerNorm kernel\n",
    "\n",
    "### **3. Graph optimization & operator scheduling**\n",
    "\n",
    "- Fusion passes\n",
    "\n",
    "- Kernel scheduling\n",
    "\n",
    "- Static vs dynamic shapes\n",
    "\n",
    "### **4. Model serving architecture**\n",
    "\n",
    "- vLLM\n",
    "\n",
    "- TensorRT-LLM\n",
    "\n",
    "- TGI\n",
    "\n",
    "- Paginated Attention\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
