{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f059b5b6",
   "metadata": {},
   "source": [
    "**THE ORDER I SHOULD STUDY (Simplified, Practical)**\n",
    "\n",
    "### **Phase 1 — Foundations (2–4 weeks)**\n",
    "\n",
    "FLOPs, GPU basics, memory hierarchy, autograd, PyTorch profiler.\n",
    "\n",
    "### **Phase 2 — Architecture Optimization (2–3 weeks)**\n",
    "\n",
    "MobileNet/EfficientNet/DistilBERT/FlashAttention.\n",
    "\n",
    "### **Phase 3 — Training Optimization (3 weeks)**\n",
    "\n",
    "FSDP, ZeRO, AMP, checkpointing, XLA.\n",
    "\n",
    "### **Phase 4 — Compression (4–6 weeks)**\n",
    "\n",
    "Quantization → pruning → distillation → LoRA → low-rank.\n",
    "\n",
    "### **Phase 5 — Mobile Deployment (3–4 weeks)**\n",
    "\n",
    "TFLite, CoreML, ONNX Mobile.\n",
    "\n",
    "### **Phase 6 — LLM Inference Optimization (4–6 weeks)**\n",
    "\n",
    "vLLM, TensorRT-LLM, KV cache, speculative decoding.\n",
    "\n",
    "### **Phase 7 — Compiler & Kernel Level (6–12 weeks)**\n",
    "\n",
    "Learn Triton → write fused kernels → study TVM/TensorRT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e19ba",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340869e",
   "metadata": {},
   "source": [
    "# **Detailed Roadmap for Model Optimization (Size, Params, Quantization)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1efc7",
   "metadata": {},
   "source": [
    "## STAGE 1 — Core Fundamentals (Must Learn First)\n",
    "\n",
    "Applies to _all_ optimization domains.\n",
    "\n",
    "### **1. FLOPs & MACs deeply**\n",
    "\n",
    "Understand:\n",
    "\n",
    "- FLOPs vs throughput\n",
    "- Memory-bound vs compute-bound\n",
    "- How to calculate FLOPs for Conv2D, MatMul, Attention\n",
    "- Why sometimes reducing parameters doesn’t reduce latency\n",
    "\n",
    "### **2. GPU Architecture**\n",
    "\n",
    "Know exactly:\n",
    "\n",
    "- Warps, threads, blocks\n",
    "- Shared memory vs global memory\n",
    "- Tensor Cores & BF16/FP16\n",
    "- Why kernel fusion matters\n",
    "\n",
    "### **3. PyTorch Internals**\n",
    "\n",
    "- TorchScript / FX Graph\n",
    "- Static vs dynamic shapes\n",
    "- JIT optimization\n",
    "- Profiling (Torch Profiler, TensorBoard)\n",
    "\n",
    "### **4. Memory Optimization Basics**\n",
    "\n",
    "- Activation checkpointing\n",
    "- Mixed precision\n",
    "- Gradient accumulation\n",
    "- Offloading (CPU/GPU/NVMe)\n",
    "\n",
    "This stage builds your foundation.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ae671",
   "metadata": {},
   "source": [
    "## STAGE 2 — Model Architecture Optimization\n",
    "\n",
    "Learn how efficient models are designed.\n",
    "\n",
    "### **CNN Efficiency (for Computer Vision)**\n",
    "\n",
    "Study:\n",
    "\n",
    "- MobileNet V1 → depthwise separable conv\n",
    "- MobileNet V2 → inverted residuals\n",
    "- MobileNet V3 → SE blocks + NAS\n",
    "- ShuffleNet → channel shuffle\n",
    "- EfficientNet → compound scaling\n",
    "- SqueezeNet → 50x smaller model design\n",
    "\n",
    "Master:\n",
    "\n",
    "- Depthwise conv math\n",
    "- Grouped conv\n",
    "- Bottleneck blocks\n",
    "- Channel pruning concepts\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformer Efficiency**\n",
    "\n",
    "Understand:\n",
    "\n",
    "- RMSNorm vs LayerNorm\n",
    "- FlashAttention\n",
    "- Multi-Query Attention (MQA)\n",
    "- Grouped Query Attention (GQA)\n",
    "- ALiBi / RoPE\n",
    "- SwiGLU vs GeLU\n",
    "- KV cache — how it fundamentally impacts latency\n",
    "\n",
    "Learn lighter architectures:\n",
    "\n",
    "- DistilBERT\n",
    "- MobileBERT\n",
    "- TinyBERT\n",
    "- Longformer / Linformer (low attention cost)\n",
    "- LLaMA architecture principles\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076380d",
   "metadata": {},
   "source": [
    "## STAGE 3 — Training-Time Optimization (Speed + Memory)\n",
    "\n",
    "This is critical.\n",
    "\n",
    "### **Techniques you must master**\n",
    "\n",
    "- FP16, BF16 (automatic mixed precision)\n",
    "- Gradient checkpointing\n",
    "- FullyShardedDataParallel (FSDP)\n",
    "- ZeRO optimizers (DeepSpeed)\n",
    "- Adafactor (low memory optimizer)\n",
    "- FlashAttention (reduces memory by 2–5×)\n",
    "- Tensor parallelism\n",
    "- Pipeline parallelism\n",
    "- Activation recomputation\n",
    "- XLA for training\n",
    "- PyTorch Compile (torch.compile)\n",
    "\n",
    "### **Your skills after this stage**\n",
    "\n",
    "You’ll know how to train:\n",
    "\n",
    "- 5× bigger models on the same GPU\n",
    "- Faster with the same hardware\n",
    "- At lower VRAM using sharding/offloading\n",
    "- With fused kernels\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df477e5",
   "metadata": {},
   "source": [
    "## STAGE 4 — Model Compression (Cross-domain)\n",
    "\n",
    "This applies to CNNs + Transformers + Mobile.\n",
    "\n",
    "### **A. Quantization**\n",
    "\n",
    "Master:\n",
    "\n",
    "- PTQ (Post-training quantization)\n",
    "- QAT (Quantization-aware training)\n",
    "- FP16, INT8, INT4, INT2\n",
    "- Weight-only quantization\n",
    "- GPTQ\n",
    "- AWQ (Activation-aware PTQ)\n",
    "- SmoothQuant\n",
    "- KV cache quantization (LLMs)\n",
    "\n",
    "Quantization is the **most important skill** for both mobile and LLM inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Pruning**\n",
    "\n",
    "Learn:\n",
    "\n",
    "- Unstructured pruning\n",
    "- Structured pruning (filters, channels, heads)\n",
    "- Movement pruning\n",
    "- Lottery ticket hypothesis\n",
    "- N:M sparsity (2:4 sparsity for Nvidia GPUs)\n",
    "\n",
    "---\n",
    "\n",
    "### **C. Knowledge Distillation**\n",
    "\n",
    "Master:\n",
    "\n",
    "- Soft targets\n",
    "- Matching intermediate layers\n",
    "- Layer dropping\n",
    "- Distilling LLMs into smaller student models\n",
    "- Distilling CNNs into mobile CNNs\n",
    "\n",
    "---\n",
    "\n",
    "### **D. Low-rank Methods**\n",
    "\n",
    "- LoRA\n",
    "- QLoRA\n",
    "- LoRA for inference distillation\n",
    "- Low-rank factorization of weight matrices\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a24f14",
   "metadata": {},
   "source": [
    "## STAGE 5 — Mobile Deployment Optimization\n",
    "\n",
    "After compressing the model, learn:\n",
    "\n",
    "### **Framework-specific**\n",
    "\n",
    "- TFLite\n",
    "- CoreML\n",
    "- ONNX Runtime Mobile\n",
    "- MediaPipe\n",
    "- MNN (Alibaba)\n",
    "- NCNN (Tencent)\n",
    "\n",
    "### **Skills**\n",
    "\n",
    "- Quantization for hardware support\n",
    "- Operator fusion\n",
    "- Delegate acceleration (GPU, NNAPI)\n",
    "- Memory footprint optimization\n",
    "- Latency vs accuracy tradeoffs\n",
    "\n",
    "### **Techniques**\n",
    "\n",
    "- Post-training integer quantization\n",
    "- Micro-Conv and GhostNet architectures\n",
    "- Pre-fused mobile kernels\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce673af1",
   "metadata": {},
   "source": [
    "## STAGE 6 — Transformers / LLM Inference Optimization\n",
    "\n",
    "This is a separate world.\n",
    "\n",
    "### **A. GPU LLM Runtimes**\n",
    "\n",
    "Learn how these work:\n",
    "\n",
    "- vLLM\n",
    "- TensorRT-LLM\n",
    "- TGI (HuggingFace)\n",
    "- DeepSpeed-MII\n",
    "- FasterTransformer\n",
    "- AITemplate\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Key Concepts**\n",
    "\n",
    "- Paginated Attention (vLLM)\n",
    "- Continuous batching\n",
    "- Prefill vs decode phase\n",
    "- KV cache optimization\n",
    "- Flash-Decoding\n",
    "- Speculative decoding\n",
    "- Quantized attention kernels\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55000c66",
   "metadata": {},
   "source": [
    "## STAGE 7 — Systems-Level Optimization (The Hardest Part)\n",
    "\n",
    "This is where you become elite.\n",
    "\n",
    "### **A. Tensor Compilers**\n",
    "\n",
    "You must learn:\n",
    "\n",
    "- ONNX Runtime\n",
    "- TensorRT kernels\n",
    "- TVM (Apache TVM)\n",
    "- MLIR\n",
    "- AITemplate\n",
    "- PyTorch 2.0 Compiler\n",
    "\n",
    "---\n",
    "\n",
    "### **B. GPU Kernel Programming**\n",
    "\n",
    "Learn Triton (way easier than CUDA):\n",
    "\n",
    "- Fused linear layers\n",
    "- Fused softmax\n",
    "- FlashAttention kernel\n",
    "- Fused LayerNorm + bias + matmul\n",
    "\n",
    "Then optionally learn basic CUDA:\n",
    "\n",
    "- Thread divergence\n",
    "- Warp-level primitives\n",
    "- Shared-memory tiling\n",
    "- Tensor core GEMM\n",
    "\n",
    "---\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef6d7b",
   "metadata": {},
   "source": [
    "## STAGE 8 — Final Mastery: Model Serving Optimization\n",
    "\n",
    "- Batching strategies\n",
    "- CPU offloading\n",
    "- Memory pools\n",
    "- Quantization-aware serving\n",
    "- Async execution queues\n",
    "\n",
    "This is what companies like OpenAI, Meta, and Google use.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
